{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alessandra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alessandra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext as sc\n",
    "import pyspark as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas1 = pd.read_csv(\"data/Airbnb_Texas_Rentals.csv\")\n",
    "texas1 = texas1.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rate_per_night</th>\n",
       "      <th>bedrooms_count</th>\n",
       "      <th>city</th>\n",
       "      <th>date_of_listing</th>\n",
       "      <th>description</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$27</td>\n",
       "      <td>2</td>\n",
       "      <td>Humble</td>\n",
       "      <td>May 2016</td>\n",
       "      <td>Welcome to stay in private room with queen bed...</td>\n",
       "      <td>30.020138</td>\n",
       "      <td>-95.293996</td>\n",
       "      <td>2 Private rooms/bathroom 10min from IAH airport</td>\n",
       "      <td>https://www.airbnb.com/rooms/18520444?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$149</td>\n",
       "      <td>4</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>November 2010</td>\n",
       "      <td>Stylish, fully remodeled home in upscale NW – ...</td>\n",
       "      <td>29.503068</td>\n",
       "      <td>-98.447688</td>\n",
       "      <td>Unique Location! Alamo Heights - Designer Insp...</td>\n",
       "      <td>https://www.airbnb.com/rooms/17481455?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$59</td>\n",
       "      <td>1</td>\n",
       "      <td>Houston</td>\n",
       "      <td>January 2017</td>\n",
       "      <td>'River house on island close to the city' \\nA ...</td>\n",
       "      <td>29.829352</td>\n",
       "      <td>-95.081549</td>\n",
       "      <td>River house near the city</td>\n",
       "      <td>https://www.airbnb.com/rooms/16926307?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$60</td>\n",
       "      <td>1</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>February 2016</td>\n",
       "      <td>Private bedroom in a cute little home situated...</td>\n",
       "      <td>30.637304</td>\n",
       "      <td>-96.337846</td>\n",
       "      <td>Private Room Close to Campus</td>\n",
       "      <td>https://www.airbnb.com/rooms/11839729?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$75</td>\n",
       "      <td>2</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>February 2017</td>\n",
       "      <td>Welcome to our original 1920's home. We recent...</td>\n",
       "      <td>32.747097</td>\n",
       "      <td>-97.286434</td>\n",
       "      <td>The Porch</td>\n",
       "      <td>https://www.airbnb.com/rooms/17325114?location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  average_rate_per_night bedrooms_count         city date_of_listing  \\\n",
       "0                    $27              2       Humble        May 2016   \n",
       "1                   $149              4  San Antonio   November 2010   \n",
       "2                    $59              1      Houston    January 2017   \n",
       "3                    $60              1        Bryan   February 2016   \n",
       "4                    $75              2   Fort Worth   February 2017   \n",
       "\n",
       "                                         description   latitude  longitude  \\\n",
       "0  Welcome to stay in private room with queen bed...  30.020138 -95.293996   \n",
       "1  Stylish, fully remodeled home in upscale NW – ...  29.503068 -98.447688   \n",
       "2  'River house on island close to the city' \\nA ...  29.829352 -95.081549   \n",
       "3  Private bedroom in a cute little home situated...  30.637304 -96.337846   \n",
       "4  Welcome to our original 1920's home. We recent...  32.747097 -97.286434   \n",
       "\n",
       "                                               title  \\\n",
       "0    2 Private rooms/bathroom 10min from IAH airport   \n",
       "1  Unique Location! Alamo Heights - Designer Insp...   \n",
       "2                          River house near the city   \n",
       "3                       Private Room Close to Campus   \n",
       "4                                          The Porch   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.airbnb.com/rooms/18520444?location...  \n",
       "1  https://www.airbnb.com/rooms/17481455?location...  \n",
       "2  https://www.airbnb.com/rooms/16926307?location...  \n",
       "3  https://www.airbnb.com/rooms/11839729?location...  \n",
       "4  https://www.airbnb.com/rooms/17325114?location...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('expand_frame_repr',False)\n",
    "texas1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create docuemnts as tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'data/docs/'\n",
    "path2 = '.tsv'\n",
    "stemmed_path='data/tokenized_docs/'\n",
    "sp = string.punctuation+'“”–’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_step(doc):\n",
    "    \"\"\"\n",
    "    takes as input the string of the document\n",
    "    removes stopwords, punctuation and makes stemming \n",
    "    \"\"\"\n",
    "    \n",
    "    # check if it's a nan value \n",
    "\n",
    "    if isinstance(doc, float):\n",
    "        return str(doc)\n",
    "    \n",
    "    doc=doc.replace(\"\\\\n\", \" \")\n",
    "    # punctuations\n",
    "    doc = [ c if c not in sp else \" \"  for c in doc ]\n",
    "    doc = ''.join(doc)\n",
    "    # stopwords\n",
    "    doc = [ word for word in doc.split() if word.lower() not in stopwords.words('english') ]\n",
    "    doc = ' '.join(doc)\n",
    "    \n",
    "    # stemming\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(doc)\n",
    "    \n",
    "    w_lst = []\n",
    "    for w in words:\n",
    "        w_lst.append(ps.stem(w))\n",
    "    \n",
    "    # something else\n",
    "    \n",
    "    return ' '.join(w_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open('data/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('data/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing create the tsv files.\n",
    "We created vocabulary parsing every document and updating it when the algorithm finds a new word. \n",
    "Format of vocabulary => 'string':integer.\n",
    "Creation of ii1:\n",
    "The first inverted index is built, updating it every time a word is find in a document.\n",
    "At the end we stored vocabulary and the inverted index using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    n=len(data)\n",
    "\n",
    "    for i in range(n):\n",
    "        with open(path1 + 'doc_'+ str(i) + '.tsv', 'w') as doc:\n",
    "            a = csv.writer(doc, delimiter='\\t')\n",
    "            a.writerow([data.iloc[i]['average_rate_per_night'],data.iloc[i]['bedrooms_count'] \n",
    "                      ,data.iloc[i]['city'] ,data.iloc[i]['date_of_listing'], data.iloc[i]['description']\n",
    "                      ,data.iloc[i]['latitude'],data.iloc[i]['longitude'] ,data.iloc[i]['title'] ,data.iloc[i]['url']])\n",
    "\n",
    "    return\n",
    "\n",
    "def create_vocabulary_and_ii1 (data):\n",
    "    n = len(data)\n",
    "    vocabulary = {}\n",
    "    ii1 = {}    \n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        # creating a tokenized string with title and description\n",
    "        tokenized_str = (remove_step(data.iloc[i]['title']) + ' ' \n",
    "                                     + remove_step(data.iloc[i]['description']))\n",
    "\n",
    "        # creating the dictionary\n",
    "        for term in tokenized_str.split(' '):\n",
    "            if term in vocabulary.keys():\n",
    "                term_id = vocabulary[term]\n",
    "            else:\n",
    "                vocabulary[term] = cnt\n",
    "                term_id = cnt\n",
    "                cnt+=1\n",
    "\n",
    "            if term_id not in ii1.keys():\n",
    "                ii1[term_id] = ['doc_'+str(i)]\n",
    "            else:\n",
    "                lista = ii1[term_id]\n",
    "                document = 'doc_'+str(i)\n",
    "                if document in lista:\n",
    "                    continue\n",
    "                else:        \n",
    "                    ii1[term_id].append('doc_'+str(i))\n",
    "\n",
    "    # store vocabulary in pickle format\n",
    "    save_obj(vocabulary, 'vocabulary')\n",
    "    save_obj(ii1, 'inverted_index_1')\n",
    "    return\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_vocabulary and inverted index 1 (for the first search engine)\n",
    "create_vocabulary_and_ii1(texas1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run if you don't want to waste time :-D \n",
    "#preprocessing(texas1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_1(query): \n",
    "    \n",
    "    query = remove_step(query)\n",
    "    query = list(set(query.split(' ')))\n",
    "    \n",
    "    lst_of_lst=[]\n",
    "    \n",
    "    vocabulary = load_obj('vocabulary')\n",
    "    ii1 = load_obj('inverted_index_1')\n",
    "    \n",
    "    for w in query:\n",
    "        if w not in vocabulary:\n",
    "            print('No results')\n",
    "            return\n",
    "        i = vocabulary[w]\n",
    "        lst_of_lst.append(ii1[i])\n",
    "\n",
    "\n",
    "    doc_list = set.intersection(*[set(sublist) for sublist in lst_of_lst])\n",
    "    doc_list = list(doc_list)\n",
    "    dl = len(doc_list)\n",
    "    \n",
    "    if dl ==0:\n",
    "        print('No results')\n",
    "        return\n",
    "\n",
    "    list_for_df=[]\n",
    "    for i in range(dl):\n",
    "        with open (\"data/docs/\" + doc_list[i] + '.tsv') as doc:\n",
    "            row = doc.read()\n",
    "            lst = row.split('\\t')\n",
    "            lst = [lst[7],lst[4],lst[2],lst[8]]\n",
    "            list_for_df.append(lst)\n",
    "        \n",
    "    df=pd.DataFrame(list_for_df, columns=['Title', 'Description', 'City', 'Url'])\n",
    "    \n",
    "    return df.head(5)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = load_obj('vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>City</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Padre Beach View Home, Walk to Beach</td>\n",
       "      <td>This 4BR, 3BA Padre Beach View Home is just st...</td>\n",
       "      <td>Corpus Christi</td>\n",
       "      <td>https://www.airbnb.com/rooms/18451940?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Key Lime Cabin</td>\n",
       "      <td>If you need a relaxing place to stay that is h...</td>\n",
       "      <td>Matagorda</td>\n",
       "      <td>https://www.airbnb.com/rooms/11312175?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Isla Del Sol beachfront condo</td>\n",
       "      <td>Isla Del Sol is a small, quiet beachfront comp...</td>\n",
       "      <td>South Padre Island</td>\n",
       "      <td>https://www.airbnb.com/rooms/12188975?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beach house on private resort</td>\n",
       "      <td>Relaxing, comfortable, and very clean home wit...</td>\n",
       "      <td>Port Isabel</td>\n",
       "      <td>https://www.airbnb.com/rooms/16338158?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Looks like Coastal Living Magazine</td>\n",
       "      <td>Gorgeous- Old world charm w modern convenience...</td>\n",
       "      <td>Galveston</td>\n",
       "      <td>https://www.airbnb.com/rooms/6119177?location=...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title  \\\n",
       "0  Padre Beach View Home, Walk to Beach   \n",
       "1                        Key Lime Cabin   \n",
       "2         Isla Del Sol beachfront condo   \n",
       "3         Beach house on private resort   \n",
       "4    Looks like Coastal Living Magazine   \n",
       "\n",
       "                                         Description                City  \\\n",
       "0  This 4BR, 3BA Padre Beach View Home is just st...      Corpus Christi   \n",
       "1  If you need a relaxing place to stay that is h...           Matagorda   \n",
       "2  Isla Del Sol is a small, quiet beachfront comp...  South Padre Island   \n",
       "3  Relaxing, comfortable, and very clean home wit...         Port Isabel   \n",
       "4  Gorgeous- Old world charm w modern convenience...           Galveston   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://www.airbnb.com/rooms/18451940?location...  \n",
       "1  https://www.airbnb.com/rooms/11312175?location...  \n",
       "2  https://www.airbnb.com/rooms/12188975?location...  \n",
       "3  https://www.airbnb.com/rooms/16338158?location...  \n",
       "4  https://www.airbnb.com/rooms/6119177?location=...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine_1('beach')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_with_tf(data):\n",
    "    vocabulary = load_obj('vocabulary')\n",
    "    n = len(data)\n",
    "    \n",
    "    ii2 = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for i in range(n):\n",
    "        tokenized_str = (remove_step(data.iloc[i]['title']) + ' ' \n",
    "                                     + remove_step(data.iloc[i]['description']))\n",
    "    \n",
    "        for term in tokenized_str.split(' '):\n",
    "            doc_name = 'doc_%s'%i\n",
    "            ii2[term].append((doc_name,1))\n",
    "        \n",
    "    return ii2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii2 = dict_with_tf(texas1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ SPARK\n",
    "nSlices = 5\n",
    "for key,values in ii2.items():\n",
    "    \n",
    "    sc = sp.SparkContext(appName = 'parallelization')\n",
    "    \n",
    "    newlst = sc.parallelize(values, nSlices)\n",
    "    \n",
    "    newlst = newlst.reduceByKey(lambda a,b:a+b)\n",
    "    \n",
    "    ii2[key] = newlst.take(len(values))\n",
    "#     need to normalize by the len of the document\n",
    "\n",
    "    sc.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(ii2,'inverted_index_onlyTF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    II with TF:\n",
    "    Taking each list for each term in the inverted index,\n",
    "    we wanted to find the occurrencies for that term in each document.\n",
    "    Thus, we created the inverted index with every document appended inside the value of each \n",
    "    term (the key of the dictionary), \n",
    "    and using in a loop the reduce_doc_list method.\n",
    "    In this method we reduced the repetitions of the same docs in each list, summing them. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_doc_list(doc_list):\n",
    "    \"\"\"\n",
    "    function called by dict_TFIDF\n",
    "    \n",
    "    It reduces the list of documents into a list \n",
    "    of tuple with doc_id and its occurencies\n",
    "    \n",
    "    input:\n",
    "    - list \n",
    "    output:\n",
    "    - list \n",
    "    \"\"\"\n",
    "    tf_term_i = Counter(doc_list)\n",
    "    doc_tf_lst = []\n",
    "    doc_tf_lst = [tuple([key,value]) for key,value in tf_term_i.items()]\n",
    "    return doc_tf_lst\n",
    "\n",
    "def compute_ii2_TFIDF(ii2,n):\n",
    "    \"\"\"\n",
    "    compute the ii2_TFIDF\n",
    "    input:\n",
    "    - inverted index matrix (with TF)\n",
    "    - number of documents\n",
    "    output:\n",
    "    - ii2 \n",
    "    \"\"\"\n",
    "    for key, value in ii2.items():\n",
    "        N = len(value)\n",
    "        new_list = []\n",
    "        for item in value:\n",
    "            new_list.append(tuple([item[0], round(float(item[1])* log(n/N),3)]))\n",
    "            \n",
    "        ii2[key] = new_list\n",
    "    return ii2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_with_TFIDF(data):\n",
    "    \"\"\"\n",
    "    creates the TFIDF inverted index as dict\n",
    "    and store it into a pickle file\n",
    "    input:\n",
    "    - data\n",
    "    \"\"\"\n",
    "    vocabulary = load_obj('vocabulary')\n",
    "    n = len(data)\n",
    "    \n",
    "    ii2 = defaultdict(list)\n",
    "    \n",
    "    for i in range(n):\n",
    "        tokenized_str = (remove_step(data.iloc[i]['title']) + ' ' \n",
    "                                     + remove_step(data.iloc[i]['description']))\n",
    "    \n",
    "        for term in tokenized_str.split(' '):\n",
    "            doc_name = 'doc_%s'%i\n",
    "            ii2[vocabulary[term]].append(doc_name)\n",
    "            \n",
    "    \n",
    "    for key,value in ii2.items():\n",
    "        ii2[key] = reduce_doc_list(value)\n",
    "    \n",
    "    ii2 = compute_ii2_TFIDF(ii2,n)\n",
    "    save_obj(ii2,'inverted_index_TFIDF')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242.7152338027954\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dict_with_TFIDF(texas1)\n",
    "print (time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii2 = load_obj('inverted_index_TFIDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prove\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrauso = {\n",
    "    0: [('doc_0', 2), ('doc_1', 1), ('doc_3', 1), ('doc_7', 1)],\n",
    "    1: [('doc_0', 2), ('doc_7', 1)]\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ii2\n",
    "temp = compute_ii2_TFIDF(temp, len(texas1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_doc_list(doc_list):\n",
    "    tf_term_i = Counter(doc_list)\n",
    "    doc_tf_lst = []\n",
    "    doc_tf_lst = [tuple([key,value]) for key,value in tf_term_i.items()]\n",
    "    return doc_tf_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {\n",
    "    0: [('doc_1',2.78), ('doc_2', 1.364), ('doc_3',2.729)],\n",
    "    1: [('doc_1',2.78), ('doc_2', 1.364)],\n",
    "    2: [('doc_3',1.354)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [0,1,1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doc_2', 4)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_doc_list(ii2['river'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d1', 2), ('d3', 1), ('d4', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_doc_list(['d1','d1','d3','d4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'d1': 2, 'd3': 1, 'd4': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(['d1','d1','d3','d4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna create a 'truth' matrix that has determined rows index as the terms of vocabulary (dicted by integers), and as columns index the documents ids.\n",
    "The matrix has stored 0 or tf-idf value in relation to the presence of i-th term in the i-th document.\n",
    "After the construction of this matrix, it will be easier to make the operations for Cosine Similarity for every query we could have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = len(vocabulary)\n",
    "cols = 18259#len(texas1)\n",
    "ii1 = load_obj('inverted_index_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(cols,rows, inv_ind):\n",
    "    matrix = np.zeros(shape = (rows,cols), dtype=float)\n",
    "    for key in inv_ind.keys():\n",
    "        res_term_id = inv_ind[key]\n",
    "        for doc in res_term_id:\n",
    "            doc_tfidf = doc[1]\n",
    "            doc_id = int(doc[0][4:])\n",
    "            matrix[doc_id][key] = doc_tfidf\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = build_matrix(rows, cols, ii2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_vector_and_take_docs(query, inv_ind_1):\n",
    "    query = remove_step(query)\n",
    "    query = query.split(' ')\n",
    "    query_vector = np.zeros(len(vocabulary))\n",
    "    iiL = len(inv_ind_1)\n",
    "    # query_vect = [  for i in range(len(vocabulary))]\n",
    "    # creating the vector of the query in [0, 1, 0, 2], the numbers depends on the occurrences of terms in the query, \n",
    "    #putted in the i-th position depending on the vocabulary\n",
    "    for el in query:\n",
    "        query_vector[vocabulary[el]] += 1        \n",
    "        compares = len(inv_ind_1[vocabulary[el]])\n",
    "        query_vector[vocabulary[el]] *= 1+log(iiL/compares)\n",
    "    #return query_vect\n",
    "    #we need to take the indices from the numpy array - tricky\n",
    "    non_zero = [el.tolist() for el in query_vector.nonzero()][0]\n",
    "    docs = set.intersection(*[set(inv_ind_1[i]) for i in non_zero])\n",
    "    docs = list(docs)\n",
    "    return query_vector, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def take_docs(query_vector, inv_ind_1):\n",
    "    #we need to take the indices from the numpy array - tricky\n",
    "    non_zero = [el.tolist() for el in query_vector.nonzero()][0]\n",
    "    docs = set.intersection(*[set(inv_ind_1[i]) for i in non_zero])\n",
    "    docs = list(docs)\n",
    "\n",
    "    return docs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec, lista = compute_query_vector_and_take_docs(\"beautiful room with view\", ii2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the matrix, the vectorized query and docs. \n",
    "We will use the matrix to take the vectorized document with all the tfidf related to each term.\n",
    "Docs need to have the presence of all the words present in the query.\n",
    "This check is done using the row of the matrix (that is the vector of the i-th document taken in consideration). \n",
    "Thus, it will be possible to compute the Cosine Similarity and first ranking of documents related to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec\n",
    "#matrix\n",
    "#lista\n",
    "def cs(query_vector, matrix, list_of_documents):  \n",
    "    docs_int_lst = []\n",
    "    #take the non zero values indexes into the query vector \n",
    "    non_zero = [el.tolist() for el in query_vector.nonzero()][0]\n",
    "     \n",
    "    for doc in list_of_documents:\n",
    "        # for each doc that contains one term of the query\n",
    "        doc_id = int(doc[4:])\n",
    "        # I take the row vector from the matrix \n",
    "        doc_vector = matrix[doc_id]\n",
    "        \n",
    "        # Taking the indexes of the ( document ) row vector related to the position of the query\n",
    "        query_words_doc = np.take(doc_vector, non_zero)\n",
    "        # take off the zero values from the row vector (I check where the document doesn't have a term of the query)\n",
    "        query_words_doc = query_words_doc.nonzero()\n",
    "        \n",
    "        query_words_doc = (query_words_doc)[0]\n",
    "        \n",
    "        if len(query_words_doc) == len(non_zero):\n",
    "            #compute cosine similarity\n",
    "            cs = 1 - spatial.distance.cosine(query_words_doc,non_zero)\n",
    "            docs_int_lst.append((doc_id, cs))\n",
    "    return docs_int_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosines = cs(vec, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosines.sort(key= lambda x:x[1], reverse =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8476, 0.9734549745236509),\n",
       " (6209, 0.9734549745236509),\n",
       " (5573, 0.9734549745236509),\n",
       " (2648, 0.9734549745236509),\n",
       " (18243, 0.9734549745236509),\n",
       " (9902, 0.9734549745236509),\n",
       " (9220, 0.9734549745236509),\n",
       " (1945, 0.9734549745236509),\n",
       " (2908, 0.9734549745236509),\n",
       " (14378, 0.9734549745236509),\n",
       " (15505, 0.9734549745236509),\n",
       " (8480, 0.9734549745236509),\n",
       " (14887, 0.9734549745236509),\n",
       " (8354, 0.9734549745236509),\n",
       " (5551, 0.9734549745236509),\n",
       " (17578, 0.9734549745236509),\n",
       " (17605, 0.9734549745236509),\n",
       " (15282, 0.9734549745236509),\n",
       " (14665, 0.9734549745236509),\n",
       " (10450, 0.9734549745236509),\n",
       " (4662, 0.9734549745236509),\n",
       " (787, 0.9734549745236509),\n",
       " (12230, 0.9734549745236509),\n",
       " (7085, 0.9734549745236509),\n",
       " (1238, 0.9734549745236509),\n",
       " (9547, 0.9734549745236509),\n",
       " (8073, 0.9734549745236509),\n",
       " (14612, 0.9734549745236509),\n",
       " (1563, 0.9734549745236509),\n",
       " (10781, 0.9734549745236509),\n",
       " (2113, 0.9734549745236509),\n",
       " (9256, 0.9734549745236509),\n",
       " (8495, 0.9734549745236509),\n",
       " (12862, 0.9734549745236509),\n",
       " (16587, 0.9734549745236509),\n",
       " (1597, 0.9734549745236509),\n",
       " (14151, 0.9734549745236509),\n",
       " (2283, 0.9734549745236509),\n",
       " (717, 0.9734549745236509),\n",
       " (12107, 0.9734549745236509),\n",
       " (11929, 0.9734549745236509),\n",
       " (12254, 0.9734549745236509),\n",
       " (15717, 0.9734549745236509),\n",
       " (6562, 0.9734549745236509),\n",
       " (4441, 0.9734549745236509),\n",
       " (11899, 0.9734549745236509),\n",
       " (15823, 0.9734549745236509),\n",
       " (14176, 0.9734549745236509),\n",
       " (12465, 0.9734549745236509),\n",
       " (8085, 0.9734549745236509),\n",
       " (16732, 0.9734549745236509),\n",
       " (13661, 0.9734549745236509),\n",
       " (6874, 0.9734549745236509),\n",
       " (1036, 0.9734549745236509),\n",
       " (12402, 0.9734549745236509),\n",
       " (3936, 0.9734549745236509),\n",
       " (13067, 0.9734549745236509),\n",
       " (2354, 0.9734549745236509),\n",
       " (13, 0.9734549745236509),\n",
       " (2865, 0.9734549745236509),\n",
       " (1723, 0.9734549745236509),\n",
       " (148, 0.9734549745236509),\n",
       " (16651, 0.9734549745236509),\n",
       " (15404, 0.9734549745236509),\n",
       " (10709, 0.9734549745236509),\n",
       " (2334, 0.9734549745236509),\n",
       " (7166, 0.9734549745236509),\n",
       " (15939, 0.9734549745236509),\n",
       " (2557, 0.9734549745236509),\n",
       " (17676, 0.9734549745236509),\n",
       " (11393, 0.9734549745236509),\n",
       " (16485, 0.9734549745236509),\n",
       " (10471, 0.9734549745236509),\n",
       " (12866, 0.9734549745236509),\n",
       " (6772, 0.9734549745236509),\n",
       " (9932, 0.9734549745236509),\n",
       " (11272, 0.9734549745236509),\n",
       " (16427, 0.9734549745236509),\n",
       " (12055, 0.9734549745236509),\n",
       " (11146, 0.9734549745236509),\n",
       " (18146, 0.9734549745236509),\n",
       " (7776, 0.9734549745236509),\n",
       " (10929, 0.9734549745236509),\n",
       " (13181, 0.9734549745236509),\n",
       " (8948, 0.9734549745236509),\n",
       " (16378, 0.9734549745236509),\n",
       " (7583, 0.9734549745236509),\n",
       " (1442, 0.9734549745236509),\n",
       " (12927, 0.9734549745236509),\n",
       " (4427, 0.9734549745236509),\n",
       " (6517, 0.9734549745236509),\n",
       " (2251, 0.9734549745236509),\n",
       " (2859, 0.9734549745236509),\n",
       " (300, 0.9734549745236509),\n",
       " (16352, 0.9734549745236509),\n",
       " (3508, 0.9734549745236509),\n",
       " (7761, 0.9734549745236509),\n",
       " (4363, 0.9734549745236509),\n",
       " (15952, 0.9734549745236509),\n",
       " (8366, 0.9734549745236509),\n",
       " (3162, 0.9734549745236509),\n",
       " (4798, 0.9734549745236509),\n",
       " (2019, 0.9734549745236509),\n",
       " (2095, 0.9734549745236509),\n",
       " (4854, 0.9734549745236509),\n",
       " (700, 0.9734549745236509),\n",
       " (8599, 0.9734549745236509),\n",
       " (5590, 0.9734549745236509),\n",
       " (8596, 0.9734549745236509),\n",
       " (5806, 0.9734549745236509),\n",
       " (2571, 0.9734549745236509),\n",
       " (2581, 0.9734549745236509),\n",
       " (11666, 0.9734549745236509),\n",
       " (9855, 0.9734549745236509),\n",
       " (18033, 0.9734549745236509),\n",
       " (4801, 0.9734549745236509),\n",
       " (13582, 0.9734549745236509),\n",
       " (7590, 0.9734549745236509),\n",
       " (10800, 0.9734549745236509),\n",
       " (9511, 0.9734549745236509),\n",
       " (8095, 0.9734549745236509),\n",
       " (1307, 0.9734549745236509),\n",
       " (7052, 0.9734549745236509),\n",
       " (5561, 0.9734549745236509),\n",
       " (8780, 0.9734549745236509),\n",
       " (673, 0.9734549745236509),\n",
       " (1395, 0.9734549745236509),\n",
       " (12851, 0.9734549745236509),\n",
       " (10059, 0.9734549745236509),\n",
       " (10638, 0.9734549745236509),\n",
       " (4055, 0.9734549745236509),\n",
       " (4456, 0.9734549745236509),\n",
       " (6625, 0.9734549745236509),\n",
       " (481, 0.9734549745236509),\n",
       " (4607, 0.9734549745236509),\n",
       " (11714, 0.9734549745236509),\n",
       " (12867, 0.9734549745236509),\n",
       " (9862, 0.9734549745236509),\n",
       " (308, 0.9734549745236509),\n",
       " (13211, 0.9734549745236509),\n",
       " (5066, 0.9734549745236509),\n",
       " (12751, 0.9734549745236509),\n",
       " (1239, 0.9734549745236509),\n",
       " (16272, 0.9734549745236509),\n",
       " (2192, 0.9734549745236509),\n",
       " (2092, 0.9734549745236509),\n",
       " (14254, 0.9734549745236509),\n",
       " (378, 0.9734549745236509),\n",
       " (5129, 0.9734549745236509),\n",
       " (5830, 0.9734549745236509),\n",
       " (8833, 0.9734549745236509),\n",
       " (2889, 0.9734549745236509),\n",
       " (2591, 0.9734549745236509),\n",
       " (11884, 0.9734549745236509),\n",
       " (8734, 0.9734549745236509),\n",
       " (2635, 0.9734549745236509),\n",
       " (16053, 0.9734549745236509),\n",
       " (9654, 0.9734549745236509),\n",
       " (16306, 0.9734549745236509),\n",
       " (15385, 0.9734549745236509),\n",
       " (13147, 0.9734549745236509),\n",
       " (10032, 0.9734549745236509),\n",
       " (6232, 0.9734549745236509),\n",
       " (482, 0.9734549745236509),\n",
       " (11362, 0.9734549745236509),\n",
       " (8988, 0.9734549745236509),\n",
       " (2669, 0.9734549745236509),\n",
       " (2327, 0.9734549745236509),\n",
       " (2165, 0.9734549745236509),\n",
       " (1867, 0.9734549745236509),\n",
       " (10738, 0.9734549745236509),\n",
       " (896, 0.9734549745236509),\n",
       " (10044, 0.9734549745236509),\n",
       " (8750, 0.9734549745236509),\n",
       " (2481, 0.9734549745236509),\n",
       " (2044, 0.9734549745236509),\n",
       " (14754, 0.9734549745236509),\n",
       " (15595, 0.9734549745236509),\n",
       " (12024, 0.9734549745236509),\n",
       " (9757, 0.9734549745236509),\n",
       " (9989, 0.9734549745236509),\n",
       " (3204, 0.9734549745236509),\n",
       " (8027, 0.9734549745236509),\n",
       " (14262, 0.9734549745236509),\n",
       " (6975, 0.9734549745236509),\n",
       " (11412, 0.9734549745236509),\n",
       " (7880, 0.9734549745236509),\n",
       " (9558, 0.9734549745236509),\n",
       " (9537, 0.9734549745236509),\n",
       " (4275, 0.9734549745236509),\n",
       " (239, 0.9734549745236509),\n",
       " (4551, 0.9734549745236509),\n",
       " (15884, 0.9734549745236509),\n",
       " (16982, 0.9734549745236509),\n",
       " (927, 0.9734549745236509),\n",
       " (12337, 0.9734549745236509),\n",
       " (13023, 0.9734549745236509),\n",
       " (17600, 0.9734549745236509),\n",
       " (10262, 0.9734549745236509),\n",
       " (14505, 0.9734549745236509),\n",
       " (4617, 0.9734549745236509),\n",
       " (4177, 0.9734549745236509),\n",
       " (4949, 0.9734549745236509),\n",
       " (17486, 0.9734549745236509),\n",
       " (8986, 0.9734549745236509),\n",
       " (2407, 0.9734549745236509),\n",
       " (4840, 0.9734549745236509),\n",
       " (11044, 0.9734549745236509),\n",
       " (2436, 0.9734549745236509),\n",
       " (14959, 0.9734549745236509),\n",
       " (4400, 0.9734549745236509),\n",
       " (6707, 0.9734549745236509),\n",
       " (13506, 0.9734549745236509),\n",
       " (2130, 0.9734549745236509),\n",
       " (6741, 0.9734549745236509),\n",
       " (1012, 0.9734549745236509)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                             Type                          Data/Info\n",
      "----------------------------------------------------------------------------\n",
      "Counter                              type                          <class 'collections.Counter'>\n",
      "PorterStemmer                        ABCMeta                       <class 'nltk.stem.porter.PorterStemmer'>\n",
      "build_matrix                         function                      <function build_matrix at 0x0000023322069950>\n",
      "cols                                 int                           18259\n",
      "compute_query_vector_and_take_docs   function                      <function compute_query_v<...>cs at 0x000002331D2F42F0>\n",
      "cs                                   function                      <function cs at 0x000002331D2F4510>\n",
      "csv                                  module                        <module 'csv' from 'C:\\\\U<...>\\Anaconda3\\\\lib\\\\csv.py'>\n",
      "defaultdict                          type                          <class 'collections.defaultdict'>\n",
      "ii1                                  dict                          n=12160\n",
      "ii2                                  defaultdict                   defaultdict(<class 'list'<...> [('doc_18252', 9.812)]})\n",
      "load_obj                             function                      <function load_obj at 0x000002331A183400>\n",
      "log                                  builtin_function_or_method    <built-in function log>\n",
      "matrix                               ndarray                       18259x12160: 222029440 elems, type `float64`, 1776235520 bytes (1693.9501953125 Mb)\n",
      "nltk                                 module                        <module 'nltk' from 'C:\\\\<...>ages\\\\nltk\\\\__init__.py'>\n",
      "non_zero                             list                          n=1\n",
      "np                                   module                        <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "path1                                str                           data/docs/\n",
      "path2                                str                           .tsv\n",
      "pd                                   module                        <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "pickle                               module                        <module 'pickle' from 'C:<...>aconda3\\\\lib\\\\pickle.py'>\n",
      "remove_step                          function                      <function remove_step at 0x000002331A183510>\n",
      "rows                                 int                           12160\n",
      "save_obj                             function                      <function save_obj at 0x000002331A183488>\n",
      "sc                                   type                          <class 'pyspark.context.SparkContext'>\n",
      "sp                                   str                           !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~“”–’\n",
      "spatial                              module                        <module 'scipy.spatial' f<...>y\\\\spatial\\\\__init__.py'>\n",
      "stemmed_path                         str                           data/tokenized_docs/\n",
      "stopwords                            WordListCorpusReader          <WordListCorpusReader in <...>ata\\\\corpora\\\\stopwords'>\n",
      "string                               module                        <module 'string' from 'C:<...>aconda3\\\\lib\\\\string.py'>\n",
      "vocabulary                           dict                          n=12160\n",
      "word_tokenize                        function                      <function word_tokenize at 0x0000023319AC70D0>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "del texas1, path1, path2, stemmed_path, sp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
