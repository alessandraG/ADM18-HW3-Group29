{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/miguel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/miguel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import math \n",
    "\n",
    "import heapq\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "#from geopy.geocoders import Nominatim\n",
    "#from geopy import distance\n",
    "\n",
    "sp = string.punctuation+'“”–’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas1 = pd.read_csv(\"data/Airbnb_Texas_Rentals.csv\")\n",
    "texas1 = texas1.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rate_per_night</th>\n",
       "      <th>bedrooms_count</th>\n",
       "      <th>city</th>\n",
       "      <th>date_of_listing</th>\n",
       "      <th>description</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$27</td>\n",
       "      <td>2</td>\n",
       "      <td>Humble</td>\n",
       "      <td>May 2016</td>\n",
       "      <td>Welcome to stay in private room with queen bed...</td>\n",
       "      <td>30.020138</td>\n",
       "      <td>-95.293996</td>\n",
       "      <td>2 Private rooms/bathroom 10min from IAH airport</td>\n",
       "      <td>https://www.airbnb.com/rooms/18520444?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$149</td>\n",
       "      <td>4</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>November 2010</td>\n",
       "      <td>Stylish, fully remodeled home in upscale NW – ...</td>\n",
       "      <td>29.503068</td>\n",
       "      <td>-98.447688</td>\n",
       "      <td>Unique Location! Alamo Heights - Designer Insp...</td>\n",
       "      <td>https://www.airbnb.com/rooms/17481455?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$59</td>\n",
       "      <td>1</td>\n",
       "      <td>Houston</td>\n",
       "      <td>January 2017</td>\n",
       "      <td>'River house on island close to the city' \\nA ...</td>\n",
       "      <td>29.829352</td>\n",
       "      <td>-95.081549</td>\n",
       "      <td>River house near the city</td>\n",
       "      <td>https://www.airbnb.com/rooms/16926307?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$60</td>\n",
       "      <td>1</td>\n",
       "      <td>Bryan</td>\n",
       "      <td>February 2016</td>\n",
       "      <td>Private bedroom in a cute little home situated...</td>\n",
       "      <td>30.637304</td>\n",
       "      <td>-96.337846</td>\n",
       "      <td>Private Room Close to Campus</td>\n",
       "      <td>https://www.airbnb.com/rooms/11839729?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$75</td>\n",
       "      <td>2</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>February 2017</td>\n",
       "      <td>Welcome to our original 1920's home. We recent...</td>\n",
       "      <td>32.747097</td>\n",
       "      <td>-97.286434</td>\n",
       "      <td>The Porch</td>\n",
       "      <td>https://www.airbnb.com/rooms/17325114?location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  average_rate_per_night bedrooms_count         city date_of_listing  \\\n",
       "0                    $27              2       Humble        May 2016   \n",
       "1                   $149              4  San Antonio   November 2010   \n",
       "2                    $59              1      Houston    January 2017   \n",
       "3                    $60              1        Bryan   February 2016   \n",
       "4                    $75              2   Fort Worth   February 2017   \n",
       "\n",
       "                                         description   latitude  longitude  \\\n",
       "0  Welcome to stay in private room with queen bed...  30.020138 -95.293996   \n",
       "1  Stylish, fully remodeled home in upscale NW – ...  29.503068 -98.447688   \n",
       "2  'River house on island close to the city' \\nA ...  29.829352 -95.081549   \n",
       "3  Private bedroom in a cute little home situated...  30.637304 -96.337846   \n",
       "4  Welcome to our original 1920's home. We recent...  32.747097 -97.286434   \n",
       "\n",
       "                                               title  \\\n",
       "0    2 Private rooms/bathroom 10min from IAH airport   \n",
       "1  Unique Location! Alamo Heights - Designer Insp...   \n",
       "2                          River house near the city   \n",
       "3                       Private Room Close to Campus   \n",
       "4                                          The Porch   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.airbnb.com/rooms/18520444?location...  \n",
       "1  https://www.airbnb.com/rooms/17481455?location...  \n",
       "2  https://www.airbnb.com/rooms/16926307?location...  \n",
       "3  https://www.airbnb.com/rooms/11839729?location...  \n",
       "4  https://www.airbnb.com/rooms/17325114?location...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('expand_frame_repr',False)\n",
    "texas1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create docuemnts as tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'data/docs/'\n",
    "path2 = '.tsv'\n",
    "stemmed_path='data/tokenized_docs/'\n",
    "sp = string.punctuation+'“”–’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_step(doc):\n",
    "    \"\"\"\n",
    "    takes as input the string of the document\n",
    "    removes stopwords, punctuation and makes stemming \n",
    "    input:\n",
    "    - string of document\n",
    "    output:\n",
    "    - list of term after stemming process\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # check if it's a nan value \n",
    "\n",
    "    if isinstance(doc, float):\n",
    "        return str(doc)\n",
    "    \n",
    "    doc=doc.replace(\"\\\\n\", \" \")\n",
    "    # punctuations\n",
    "    doc = [ c if c not in sp else \" \" for c in doc ]\n",
    "    doc = ''.join(doc)\n",
    "    # stopwords\n",
    "    doc = [ word for word in doc.split() if word.lower() not in stopwords.words('english') ]\n",
    "    doc = ' '.join(doc)\n",
    "    \n",
    "    # stemming\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(doc)\n",
    "    \n",
    "    w_lst = []\n",
    "    for w in words:\n",
    "        w_lst.append(ps.stem(w))\n",
    "    \n",
    "    # something else\n",
    "    \n",
    "    return ' '.join(w_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    \"\"\"\n",
    "    save the object int a pickle file\n",
    "    \"\"\"\n",
    "    with open('data/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    \"\"\"\n",
    "    load the object from a pickle file\n",
    "    \"\"\"\n",
    "    with open('data/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing create the tsv files.\n",
    "We created vocabulary parsing every document and updating it when the algorithm finds a new word. \n",
    "Format of vocabulary => 'string':integer.\n",
    "Creation of ii1:\n",
    "The first inverted index is built, updating it every time a word is find in a document.\n",
    "At the end we stored vocabulary and the inverted index using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    n=len(data)\n",
    "\n",
    "    for i in range(n):\n",
    "        with open(path1 + 'doc_'+ str(i) + '.tsv', 'w', encoding = \"utf-8\") as doc:\n",
    "            a = csv.writer(doc, delimiter='\\t')\n",
    "            a.writerow([data.iloc[i]['average_rate_per_night'],data.iloc[i]['bedrooms_count'], data.iloc[i]['city'],\n",
    "                        data.iloc[i]['date_of_listing'], data.iloc[i]['description'],\n",
    "                        data.iloc[i]['latitude'],\n",
    "                        data.iloc[i]['longitude'],\n",
    "                        data.iloc[i]['title'],\n",
    "                        data.iloc[i]['url']])\n",
    "\n",
    "    return\n",
    "\n",
    "def create_vocabulary_and_ii1 (data):\n",
    "    n = len(data)\n",
    "    vocabulary = {}\n",
    "    ii1 = {}    \n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        # creating a tokenized string with title and description\n",
    "        tokenized_str = (remove_step(data.iloc[i]['title']) + ' ' \n",
    "                                     + remove_step(data.iloc[i]['description']))\n",
    "\n",
    "        # creating the dictionary\n",
    "        for term in tokenized_str.split(' '):\n",
    "            if term in vocabulary.keys():\n",
    "                term_id = vocabulary[term]\n",
    "            else:\n",
    "                vocabulary[term] = cnt\n",
    "                term_id = cnt\n",
    "                cnt+=1\n",
    "\n",
    "            if term_id not in ii1.keys():\n",
    "                ii1[term_id] = ['doc_'+str(i)]\n",
    "            else:\n",
    "                lista = ii1[term_id]\n",
    "                document = 'doc_'+str(i)\n",
    "                if document in lista:\n",
    "                    continue\n",
    "                else:        \n",
    "                    ii1[term_id].append('doc_'+str(i))\n",
    "\n",
    "    # store vocabulary in pickle format\n",
    "    save_obj(vocabulary, 'vocabulary')\n",
    "    save_obj(ii1, 'inverted_index_1')\n",
    "    return\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_vocabulary and inverted index 1 (for the first search engine)\n",
    "create_vocabulary_and_ii1(texas1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not run if you don't want to waste time :-D \n",
    "preprocessing(texas1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_1(query): \n",
    "    \n",
    "    query = remove_step(query)\n",
    "    query = list(set(query.split(' ')))\n",
    "    \n",
    "    lst_of_lst=[]\n",
    "    \n",
    "    vocabulary = load_obj('vocabulary')\n",
    "    ii1 = load_obj('inverted_index_1')\n",
    "    \n",
    "    for w in query:\n",
    "        if w not in vocabulary:\n",
    "            print('No results')\n",
    "            return\n",
    "        i = vocabulary[w]\n",
    "        lst_of_lst.append(ii1[i])\n",
    "\n",
    "\n",
    "    doc_list = set.intersection(*[set(sublist) for sublist in lst_of_lst])\n",
    "    doc_list = list(doc_list)\n",
    "    dl = len(doc_list)\n",
    "    \n",
    "    if dl ==0:\n",
    "        print('No results')\n",
    "        return\n",
    "\n",
    "    list_for_df=[]\n",
    "    for i in range(dl):\n",
    "        with open (\"data/docs/\" + doc_list[i] + '.tsv') as doc:\n",
    "            row = doc.read()\n",
    "            lst = row.split('\\t')\n",
    "            lst = [lst[7],lst[4],lst[2],lst[8]]\n",
    "            list_for_df.append(lst)\n",
    "        \n",
    "    df=pd.DataFrame(list_for_df, columns=['Title', 'Description', 'City', 'Url'])\n",
    "    \n",
    "    return df.head(5)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = load_obj('vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>City</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Vintage room in Fort Worth</td>\n",
       "      <td>Our place is a beautiful cozy open concept hou...</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>https://www.airbnb.com/rooms/18959678?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Vintage room in Fort Worth</td>\n",
       "      <td>Our place is a beautiful cozy open concept hou...</td>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>https://www.airbnb.com/rooms/18959678?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Colorful Garden Home plus Private Studio Apart...</td>\n",
       "      <td>Natural beauty and great art make this clean, ...</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>https://www.airbnb.com/rooms/17341454?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simple room at centralized location</td>\n",
       "      <td>Private room at centralized location. The rest...</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>https://www.airbnb.com/rooms/4196708?location=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beautiful Condo. Direct access to the Beach.</td>\n",
       "      <td>Beautiful Condo with direct access the South P...</td>\n",
       "      <td>South Padre Island</td>\n",
       "      <td>https://www.airbnb.com/rooms/17045040?location...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                     The Vintage room in Fort Worth   \n",
       "1                     The Vintage room in Fort Worth   \n",
       "2  Colorful Garden Home plus Private Studio Apart...   \n",
       "3                Simple room at centralized location   \n",
       "4       Beautiful Condo. Direct access to the Beach.   \n",
       "\n",
       "                                         Description                City  \\\n",
       "0  Our place is a beautiful cozy open concept hou...          Fort Worth   \n",
       "1  Our place is a beautiful cozy open concept hou...          Fort Worth   \n",
       "2  Natural beauty and great art make this clean, ...         San Antonio   \n",
       "3  Private room at centralized location. The rest...              Dallas   \n",
       "4  Beautiful Condo with direct access the South P...  South Padre Island   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://www.airbnb.com/rooms/18959678?location...  \n",
       "1  https://www.airbnb.com/rooms/18959678?location...  \n",
       "2  https://www.airbnb.com/rooms/17341454?location...  \n",
       "3  https://www.airbnb.com/rooms/4196708?location=...  \n",
       "4  https://www.airbnb.com/rooms/17045040?location...  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine_1('Beautiful room with garden')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_with_tf(data):\n",
    "    vocabulary = load_obj('vocabulary')\n",
    "    n = len(data)\n",
    "    \n",
    "    ii2 = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for i in range(n):\n",
    "        tokenized_str = (remove_step(data.iloc[i]['title']) + ' ' \n",
    "                                     + remove_step(data.iloc[i]['description']))\n",
    "    \n",
    "        for term in tokenized_str.split(' '):\n",
    "            doc_name = 'doc_%s'%i\n",
    "            ii2[term].append((doc_name,1))\n",
    "        \n",
    "    return ii2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii2 = dict_with_tf(texas1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(ii2,'inverted_index_onlyTF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    II with TF:\n",
    "    Taking each list for each term in the inverted index,\n",
    "    we wanted to find the occurrencies for that term in each document.\n",
    "    Thus, we created the inverted index with every document appended inside the value of each \n",
    "    term (the key of the dictionary), \n",
    "    and using in a loop the reduce_doc_list method.\n",
    "    In this method we reduced the repetitions of the same docs in each list, summing them. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_doc_list(doc_list):\n",
    "    \"\"\"\n",
    "    function called by dict_TFIDF\n",
    "    \n",
    "    It reduces the list of documents into a list \n",
    "    of tuple with doc_id and its occurencies\n",
    "    \n",
    "    input:\n",
    "    - list \n",
    "    output:\n",
    "    - list \n",
    "    \"\"\"\n",
    "    tf_term_i = Counter(doc_list)\n",
    "    doc_tf_lst = []\n",
    "    doc_tf_lst = [tuple([key,value]) for key,value in tf_term_i.items()]\n",
    "    return doc_tf_lst\n",
    "\n",
    "def compute_ii2_TFIDF(ii2,n):\n",
    "    \"\"\"\n",
    "    compute the ii2_TFIDF\n",
    "    input:\n",
    "    - inverted index matrix (with TF)\n",
    "    - number of documents\n",
    "    output:\n",
    "    - ii2 \n",
    "    \"\"\"\n",
    "    for key, value in ii2.items():\n",
    "        N = len(value)\n",
    "        new_list = []\n",
    "        for item in value:\n",
    "            new_list.append(tuple([item[0], round(float(item[1])* log(n/N),3)]))\n",
    "            \n",
    "        ii2[key] = new_list\n",
    "    return ii2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_with_TFIDF(data):\n",
    "    \"\"\"\n",
    "    creates the TFIDF inverted index as dict\n",
    "    and store it into a pickle file\n",
    "    input:\n",
    "    - data\n",
    "    \"\"\"\n",
    "    vocabulary = load_obj('vocabulary')\n",
    "    n = len(data)\n",
    "    \n",
    "    ii2 = defaultdict(list)\n",
    "    \n",
    "    for i in range(n):\n",
    "        tokenized_str = (remove_step(data.iloc[i]['title']) + ' ' \n",
    "                                     + remove_step(data.iloc[i]['description']))\n",
    "    \n",
    "        for term in tokenized_str.split(' '):\n",
    "            doc_name = 'doc_%s'%i\n",
    "            ii2[vocabulary[term]].append(doc_name)\n",
    "            \n",
    "    \n",
    "    for key,value in ii2.items():\n",
    "        ii2[key] = reduce_doc_list(value)\n",
    "    \n",
    "    ii2 = compute_ii2_TFIDF(ii2,n)\n",
    "    save_obj(ii2,'inverted_index_TFIDF')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242.7152338027954\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dict_with_TFIDF(texas1)\n",
    "print (time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to create the new search engine (second) basing on the inverted indexes already built, but cutting them: in this way the laptop will have less computation than to compute everything.\n",
    "Thus, every time we have a new query, the method generates 2 different inverted indexes.\n",
    "\n",
    "In this way, we won't need a big numpy array for the query and each document considered (we took docs that contained every word, using intersection (for this purpose we took the first inverted index)) for making the cosine similarity (for this we exploited the second inverted index) ; in fact, vectors for computing cosine similarity will be exactly long like the length of the query. \n",
    "Query will have 1 for each element (?)(still to decide) and doc vector will have each tfidf for each term of the query.\n",
    "\n",
    "$$ 1-cosine(\\text{doc array for the query},\\text{array of query}) $$\n",
    "will compute the Cosine Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_2(query, k = 5):\n",
    "    \"\"\"\n",
    "    search engine with TFIDF\n",
    "    input:\n",
    "    - query: the query (string)\n",
    "    - k: number of output as doc (default = 5)\n",
    "    output:\n",
    "    - df: dataframe with k documents\n",
    "    \"\"\"\n",
    "\n",
    "    # stemming the query with remove step method\n",
    "    query = remove_step(query).split(' ')\n",
    "    vocabulary = load_obj('vocabulary')\n",
    "    \n",
    "    # filtering the values not contained in vocabulary\n",
    "    # dict and mapping each term to its term_ID value\n",
    "    query = filter(lambda x: x in vocabulary.keys(),query)\n",
    "    query = list(map(lambda x: vocabulary[x], query))\n",
    "    \n",
    "    # making query list and array at the same time \n",
    "    # with Counter() dict (to ensure the order of the 2 arrays)\n",
    "    # (dictionaries are not ordered)\n",
    "    query_array = []\n",
    "    query_list = []\n",
    "    for key, frequence in Counter(query).items():\n",
    "        query_list.append(key)\n",
    "        query_array.append(frequence)\n",
    "    \n",
    "    # creating query list and numpy array with frequencies\n",
    "    query = query_list\n",
    "    query_array = np.array([query_array])\n",
    "\n",
    "    # list of documents obtained intersecating each list\n",
    "    # from inverted index 1 (easier to compute)\n",
    "    docs = []\n",
    "    ii1 = load_obj('inverted_index_1')\n",
    "    ii1 = {term_id:ii1[term_id] for term_id in query}\n",
    "    docs = set.intersection(*[set(value) for value in ii1.values()])\n",
    "\n",
    "    # filtering ii2 with only the query terms\n",
    "    ii2 = load_obj('inverted_index_TFIDF')\n",
    "    ii2 = {term_id:ii2[term_id] for term_id in query}\n",
    "    \n",
    "    # for each doc in the list docs it creates the ranking list\n",
    "    rank_lst = []\n",
    "    for doc_id in docs:\n",
    "        # doc TFIDF array. It must have the same 'order' as\n",
    "        # the query array. to ensure this we use the query list\n",
    "        array = []\n",
    "        for item in query:\n",
    "            array += tuple(filter(lambda x:x[0]==doc_id, ii2[item]))\n",
    "\n",
    "        array = np.array([x[1] for x in array])\n",
    "        rank_lst.append(tuple([1-cosine(array,query_array),doc_id]))\n",
    "    \n",
    "    # print('%d documents'%len(rank_lst)) DEBUG\n",
    "    \n",
    "    df = first_k_documents(rank_lst, 5)\n",
    "    \n",
    "    return len(rank_lst)\n",
    "    return df\n",
    "\n",
    "\n",
    "def first_k_documents(heap_rank_lst, k = 5):\n",
    "    \"\"\"\n",
    "    it returns the first k documents as a df\n",
    "    loading them into an heap memory\n",
    "    input:\n",
    "    - heap_rank_lst: list to be heaped\n",
    "    - k: first k documents\n",
    "    output:\n",
    "    - df: dataframe with k documents\n",
    "    \"\"\"\n",
    "    list_for_df = []\n",
    "    \n",
    "    if k > len(heap_rank_lst):\n",
    "        k = len(heap_rank_lst)\n",
    "        \n",
    "    heapq._heapify_max(heap_rank_lst)\n",
    "    \n",
    "    for i in range(k):\n",
    "        current_doc = heapq._heappop_max(heap_rank_lst)\n",
    "    \n",
    "        with open (\"data/docs/\" + current_doc[1] + '.tsv') as doc:\n",
    "            row = doc.read()\n",
    "            lst = row.split('\\t')\n",
    "            lst = [lst[7],lst[4],lst[2],lst[8], round(current_doc[0],3)]\n",
    "            list_for_df.append(lst)\n",
    "    \n",
    "    return pd.DataFrame(list_for_df, columns=['Title', 'Description', 'City', 'Url', 'Similarity'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6097888946533203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "df = search_engine_2(\"search engine\")\n",
    "print(time()-start)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms_ranker = {\n",
    "    0: 1,\n",
    "    1: 0.7,\n",
    "    2: 0.4,\n",
    "    3: 0.1,\n",
    "    # others values are almost zero! \n",
    "}\n",
    "\n",
    "def rooms_rank (rooms, nrooms, rooms_ranker):\n",
    "    \"\"\"\n",
    "    ranks the room\n",
    "    \"\"\"\n",
    "    \n",
    "    # if rooms or nrooms is a string (eg: studio)\n",
    "    # convert it into a string\n",
    "    if isinstance(rooms, str):\n",
    "        rooms = 1\n",
    "    if isinstance(nrooms, str):\n",
    "        nrooms = 1\n",
    "    \n",
    "    # taking the distance between the two rooms\n",
    "    value = abs(rooms - nrooms)\n",
    "\n",
    "    # if there's no a region of values give zero\n",
    "    if value not in rooms_ranker:\n",
    "        return 0\n",
    "    else:\n",
    "        # give the value signed in rooms_ranker\n",
    "        return rooms_ranker[value]\n",
    "\n",
    "def price_rank (p_query, p_doc):\n",
    "    \n",
    "    if p_query == p_doc:\n",
    "        return 1\n",
    "    \n",
    "    if p_query > p_doc:\n",
    "        return p_doc/p_query\n",
    "    else:\n",
    "        return p_query/p_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other price raker values! \n",
    "# if we don't know how to handle the problem\n",
    "price_ranker = {\n",
    "    \n",
    "    10: 1,\n",
    "    20: 0.8,\n",
    "    40: 0.6,\n",
    "    60: 0.4,\n",
    "    80: 0.2   \n",
    "}\n",
    "\n",
    "A = 10\n",
    "B = 20 \n",
    "C = 40 \n",
    "D = 60\n",
    "E = 80\n",
    "\n",
    "def rooms_rank (n_query, n_doc, rooms_ranker):\n",
    "    value = abs(n_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_3(query): \n",
    "    \n",
    "    query = remove_step(query)\n",
    "    query = list(set(query.split(' ')))\n",
    "    \n",
    "    heap = []\n",
    "    lst_of_lst=[]\n",
    "    \n",
    "    vocabulary = load_obj('vocabulary')\n",
    "    ii1 = load_obj('inverted_index_1')\n",
    "    \n",
    "    for w in query:\n",
    "        if w not in vocabulary:\n",
    "            print('No results')\n",
    "            return\n",
    "        i = vocabulary[w]\n",
    "        lst_of_lst.append(ii1[i])\n",
    "\n",
    "\n",
    "    doc_list = set.intersection(*[set(sublist) for sublist in lst_of_lst])\n",
    "    doc_list = list(doc_list)\n",
    "    dl = len(doc_list)   \n",
    "    \n",
    "    if dl == 0:\n",
    "        print('No results')\n",
    "        return\n",
    "    \n",
    "    print(\"max price:\")\n",
    "    rmax = int(input())\n",
    "    print(\"n rooms:\")\n",
    "    rooms = int(input())\n",
    "    #if len(str(rmin)) == 0:\n",
    "    #    rmin = 1\n",
    "    for doc in doc_list:\n",
    "        with open('data/docs/'+doc+'.tsv', encoding = 'utf-8') as f:\n",
    "            row = f.read()\n",
    "            row = row.split('\\t')\n",
    "            dp = int(row[0][1:])\n",
    "            try:\n",
    "                nrooms = int(row[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if math.isnan(float(row[1])):\n",
    "                continue\n",
    "\n",
    "             # and dp > rmin: \n",
    "            #sp = abs(rmax - rmin)/dp\n",
    "            score_price = 0.8*price_rank(rmax,dp)\n",
    "            ratio_rooms = 0.2*rooms_rank(rooms, nrooms, rooms_ranker)\n",
    "                #if ratio_rooms <= 1 and ratio_rooms > 0.5 and sp > 0.5 and sp <=1:  \n",
    "            heappush(heap,tuple([score_price+ratio_rooms,doc])) \n",
    "    \n",
    "    list_for_df=[]\n",
    "    k=5\n",
    "    \n",
    "    l = len(heap)\n",
    "    for i in range(l):\n",
    "        heapq._heapify_max(heap)\n",
    "        tup = heappop(heap)\n",
    "        #print(tup)\n",
    "        with open (\"data/docs/\" + tup[1] + '.tsv', encoding = 'utf-8') as doc:\n",
    "            row = doc.read()\n",
    "            lst = row.split('\\t')\n",
    "            lst = [i, lst[7],lst[4],lst[2],lst[8]]\n",
    "            list_for_df.append(lst)\n",
    "        \n",
    "    df=pd.DataFrame(list_for_df, columns=['Ranking','Title', 'Description', 'City', 'Url'])\n",
    "    \n",
    "    return df.head(k),tup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max price:\n",
      "3\n",
      "n rooms:\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "res,tup = search_engine_3(\"texas room private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_distance(center, house):\n",
    "      \n",
    "    #R = distance.distance(center, house).km\n",
    "    #print(R)\n",
    "    R  = 6\n",
    "    #print(R)\n",
    "    \n",
    "    if R <= 1: \n",
    "        return 1\n",
    "    else: \n",
    "        #return 1 - R / (1 + R)\n",
    "        #return 1 - (math.log(R)  + 1) / (1 + (math.log(R) + 1))\n",
    "        return 1- ((R*1/2 + 1) / (1 + (R*1/2 + 1)))\n",
    "        #return 3**(-R)# 1/R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Nominatim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-6721834fb443>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgeolocator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNominatim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"specify_your_app_name_here\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'where : '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeolocator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeocode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlongitude\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Nominatim' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
    "location = input('where : ')\n",
    "location = geolocator.geocode(location)\n",
    "center = [location.latitude, location.longitude]\n",
    "\n",
    "docs = ['doc_2']\n",
    "for i in range(len(docs)):\n",
    "\n",
    "    with open (\"data/docs/\" + str(docs[i]) + '.tsv') as doc:\n",
    "\n",
    "        row = doc.read()\n",
    "        row = row.split('\\t')\n",
    "        house = [float(row[5]),float(row[6])]\n",
    "        \n",
    "        \n",
    "        if math.isnan(house[0]):\n",
    "                # extract the location from the name of the city\n",
    "                location = geolocator.geocode(row[2])\n",
    "                house = [location.latitude, location.longitude]\n",
    "\n",
    "        print(scoring_distance(center, house))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
